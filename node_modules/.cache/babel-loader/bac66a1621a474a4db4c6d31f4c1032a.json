{"ast":null,"code":"var _jsxFileName = \"/Users/matt/stackathon/src/App.js\",\n    _s = $RefreshSig$();\n\n//install dependencies DONE\n//import dependiencies DONE\n//setup webcam and canvas DONE\n//define references to those DONE\n//load facemesh DONE\n//detect function DONE\n//drawing utilities\n//load triangulation\n//setup triangle path\n//setup point drawing\n//add drawmesh to detect function\nimport { useRef } from 'react'; //import logo from './logo.svg';\n\nimport './App.css';\nimport * as tf from \"@tensorflow/tfjs\";\nimport * as faceLandmarksDetection from '@tensorflow-models/face-landmarks-detection'; //at one point i was suppose to use facemesh but this no longer is supported\n//import * as facemesh from \"@tensorflow-models/facemesh\";\n\nimport Webcam from \"react-webcam\";\nimport { drawMesh } from \"./utilities\";\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\nfunction App() {\n  _s();\n\n  //set up references\n  const webcamRef = useRef(null);\n  const canvasRef = useRef(null); // load facemesh\n\n  const runFacemesh = async () => {\n    if (webcamRef !== null) {\n      const net = await faceLandmarksDetection.load(faceLandmarksDetection.SupportedPackages.mediapipeFacemesh);\n      setInterval(async () => {\n        if (webcamRef) {\n          await main(net);\n        }\n      }, 150);\n    }\n  };\n  /*\n    // detect function\n    const detect = async(net) =>{\n      //check to make sure webcame is ready for detection\n      if(\n        typeof webcamRef.currrent !== \"undefined\" && webcamRef.current !== null && webcamRef.current.video.readyState===4){\n          //get video properties\n          const video = webcamRef.current.video;\n          const videoWidth = webcamRef.current.video.videoWidth;\n          const videoHeight = webcamRef.current.video.videoHeight;\n          //set video width\n          webcamRef.current.video.width = videoWidth;\n          webcamRef.current.video.height = videoHeight\n          //set canvas width\n          canvasRef.current.width = videoWidth;\n          canvasRef.current.height = videoHeight;\n          //make detections\n          const face = await net.estimateFaces(video);\n          if (face){\n            console.log(face)\n          }\n          // get canvas context for drawing\n        }\n    }\n  \n    runFacemesh();\n    */\n\n\n  async function main(model) {\n    const video = webcamRef.current.video;\n    const videoWidth = webcamRef.current.video.videoWidth;\n    const videoHeight = webcamRef.current.video.videoHeight; //set video width\n\n    webcamRef.current.video.width = videoWidth;\n    webcamRef.current.video.height = videoHeight; //set canvas width\n\n    canvasRef.current.width = videoWidth;\n    canvasRef.current.height = videoHeight; // Load the MediaPipe Facemesh package.\n\n    if (webcamRef !== null) {\n      // Pass in a video stream (or an image, canvas, or 3D tensor) to obtain an\n      // array of detected faces from the MediaPipe graph. If passing in a video\n      // stream, a single prediction per frame will be returned.\n      const predictions = await model.estimateFaces({\n        input: video,\n        predictIrises: true\n      });\n      predictions.forEach(prediction => {//console.log(prediction.scaledMesh)\n      });\n\n      if (predictions.length > 0) {\n        const ctx = canvasRef.current.getContext(\"2d\");\n        drawMesh(predictions, ctx);\n        /*\n        `predictions` is an array of objects describing each detected face, for example:\n         [\n          {\n            faceInViewConfidence: 1, // The probability of a face being present.\n            boundingBox: { // The bounding box surrounding the face.\n              topLeft: [232.28, 145.26],\n              bottomRight: [449.75, 308.36],\n            },\n            mesh: [ // The 3D coordinates of each facial landmark.\n              [92.07, 119.49, -17.54],\n              [91.97, 102.52, -30.54],\n              ...\n            ],\n            scaledMesh: [ // The 3D coordinates of each facial landmark, normalized.\n              [322.32, 297.58, -17.54],\n              [322.18, 263.95, -30.54]\n            ],\n            annotations: { // Semantic groupings of the `scaledMesh` coordinates.\n              silhouette: [\n                [326.19, 124.72, -3.82],\n                [351.06, 126.30, -3.00],\n                ...\n              ],\n              ...\n            }\n          }\n        ]\n        */\n\n        for (let i = 0; i < predictions.length; i++) {\n          const keypoints = predictions[i].scaledMesh;\n          const header = document.getElementById('webcam');\n          const elementPosition = header.getBoundingClientRect();\n          console.log(elementPosition); // Log facial keypoints.\n\n          for (let i = 0; i < keypoints.length; i++) {\n            const [x, y, z] = keypoints[i]; //  console.log(`Keypoint ${i}: [${x}, ${y}, ${z}]`);\n          }\n        }\n      }\n    }\n  }\n\n  if (webcamRef) {\n    runFacemesh();\n  }\n\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    className: \"App\",\n    children: /*#__PURE__*/_jsxDEV(\"header\", {\n      className: \"App-header\",\n      children: [/*#__PURE__*/_jsxDEV(Webcam, {\n        id: \"webcam\",\n        ref: webcamRef,\n        style: {\n          position: \"absolute\",\n          marginLeft: \"auto\",\n          marginRight: \"auto\",\n          left: 0,\n          right: 0,\n          textAlign: \"center\",\n          zIndex: 9,\n          width: 640,\n          height: 480\n        }\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 150,\n        columnNumber: 7\n      }, this), /*#__PURE__*/_jsxDEV(\"canvas\", {\n        ref: canvasRef,\n        style: {\n          position: \"absolute\",\n          marginLeft: \"auto\",\n          marginRight: \"auto\",\n          left: 0,\n          right: 0,\n          textAlign: \"center\",\n          zIndex: 9,\n          width: 640,\n          height: 480\n        }\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 163,\n        columnNumber: 7\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 149,\n      columnNumber: 7\n    }, this)\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 148,\n    columnNumber: 5\n  }, this);\n}\n\n_s(App, \"AwQWgsmsPhWgADiRou0jnDEtoH4=\");\n\n_c = App;\nexport default App;\n\nvar _c;\n\n$RefreshReg$(_c, \"App\");","map":{"version":3,"sources":["/Users/matt/stackathon/src/App.js"],"names":["useRef","tf","faceLandmarksDetection","Webcam","drawMesh","App","webcamRef","canvasRef","runFacemesh","net","load","SupportedPackages","mediapipeFacemesh","setInterval","main","model","video","current","videoWidth","videoHeight","width","height","predictions","estimateFaces","input","predictIrises","forEach","prediction","length","ctx","getContext","i","keypoints","scaledMesh","header","document","getElementById","elementPosition","getBoundingClientRect","console","log","x","y","z","position","marginLeft","marginRight","left","right","textAlign","zIndex"],"mappings":";;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA,SAAQA,MAAR,QAAqB,OAArB,C,CACA;;AACA,OAAO,WAAP;AACA,OAAO,KAAKC,EAAZ,MAAoB,kBAApB;AACA,OAAO,KAAKC,sBAAZ,MAAwC,6CAAxC,C,CACA;AACA;;AACA,OAAOC,MAAP,MAAmB,cAAnB;AACA,SAAQC,QAAR,QAAuB,aAAvB;;;AAGA,SAASC,GAAT,GAAe;AAAA;;AACb;AACA,QAAMC,SAAS,GAAGN,MAAM,CAAC,IAAD,CAAxB;AACA,QAAMO,SAAS,GAAGP,MAAM,CAAE,IAAF,CAAxB,CAHa,CAMb;;AACA,QAAMQ,WAAW,GAAG,YAAW;AAC7B,QAAGF,SAAS,KAAK,IAAjB,EAAsB;AACtB,YAAMG,GAAG,GAAG,MAAMP,sBAAsB,CAACQ,IAAvB,CAA4BR,sBAAsB,CAACS,iBAAvB,CAAyCC,iBAArE,CAAlB;AAEAC,MAAAA,WAAW,CAAC,YAAU;AACpB,YAAGP,SAAH,EAAa;AACb,gBAAMQ,IAAI,CAACL,GAAD,CAAV;AACC;AACF,OAJU,EAIT,GAJS,CAAX;AAKD;AACA,GAVD;AAWF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACE,iBAAeK,IAAf,CAAoBC,KAApB,EAA2B;AACvB,UAAMC,KAAK,GAAGV,SAAS,CAACW,OAAV,CAAkBD,KAAhC;AACE,UAAME,UAAU,GAAGZ,SAAS,CAACW,OAAV,CAAkBD,KAAlB,CAAwBE,UAA3C;AACA,UAAMC,WAAW,GAAGb,SAAS,CAACW,OAAV,CAAkBD,KAAlB,CAAwBG,WAA5C,CAHqB,CAIrB;;AACAb,IAAAA,SAAS,CAACW,OAAV,CAAkBD,KAAlB,CAAwBI,KAAxB,GAAgCF,UAAhC;AACAZ,IAAAA,SAAS,CAACW,OAAV,CAAkBD,KAAlB,CAAwBK,MAAxB,GAAiCF,WAAjC,CANqB,CAOrB;;AACAZ,IAAAA,SAAS,CAACU,OAAV,CAAkBG,KAAlB,GAA0BF,UAA1B;AACAX,IAAAA,SAAS,CAACU,OAAV,CAAkBI,MAAlB,GAA2BF,WAA3B,CATqB,CAUzB;;AACA,QAAIb,SAAS,KAAG,IAAhB,EAAqB;AAErB;AACA;AACA;AACA,YAAMgB,WAAW,GAAG,MAAMP,KAAK,CAACQ,aAAN,CAAoB;AAC5CC,QAAAA,KAAK,EAAER,KADqC;AAC9BS,QAAAA,aAAa,EAAC;AADgB,OAApB,CAA1B;AAGAH,MAAAA,WAAW,CAACI,OAAZ,CAAoBC,UAAU,IAAE,CAC9B;AACD,OAFD;;AAIA,UAAIL,WAAW,CAACM,MAAZ,GAAqB,CAAzB,EAA4B;AAC1B,cAAMC,GAAG,GAAGtB,SAAS,CAACU,OAAV,CAAkBa,UAAlB,CAA6B,IAA7B,CAAZ;AACF1B,QAAAA,QAAQ,CAACkB,WAAD,EAAcO,GAAd,CAAR;AACE;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAGM,aAAK,IAAIE,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGT,WAAW,CAACM,MAAhC,EAAwCG,CAAC,EAAzC,EAA6C;AAC3C,gBAAMC,SAAS,GAAGV,WAAW,CAACS,CAAD,CAAX,CAAeE,UAAjC;AACA,gBAAMC,MAAM,GAAGC,QAAQ,CAACC,cAAT,CAAwB,QAAxB,CAAf;AACA,gBAAMC,eAAe,GAAGH,MAAM,CAACI,qBAAP,EAAxB;AACAC,UAAAA,OAAO,CAACC,GAAR,CAAYH,eAAZ,EAJ2C,CAM3C;;AACA,eAAK,IAAIN,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGC,SAAS,CAACJ,MAA9B,EAAsCG,CAAC,EAAvC,EAA2C;AACzC,kBAAM,CAACU,CAAD,EAAIC,CAAJ,EAAOC,CAAP,IAAYX,SAAS,CAACD,CAAD,CAA3B,CADyC,CAG3C;AACC;AACF;AACF;AACF;AACA;;AACD,MAAIzB,SAAJ,EAAc;AACdE,IAAAA,WAAW;AACV;;AAGD,sBACE;AAAK,IAAA,SAAS,EAAC,KAAf;AAAA,2BACE;AAAQ,MAAA,SAAS,EAAC,YAAlB;AAAA,8BACA,QAAC,MAAD;AAAQ,QAAA,EAAE,EAAC,QAAX;AAAoB,QAAA,GAAG,EAAEF,SAAzB;AAAoC,QAAA,KAAK,EACvC;AACAsC,UAAAA,QAAQ,EAAC,UADT;AAEAC,UAAAA,UAAU,EAAE,MAFZ;AAGAC,UAAAA,WAAW,EAAE,MAHb;AAIAC,UAAAA,IAAI,EAAE,CAJN;AAKAC,UAAAA,KAAK,EAAE,CALP;AAMAC,UAAAA,SAAS,EAAE,QANX;AAOAC,UAAAA,MAAM,EAAE,CAPR;AAQA9B,UAAAA,KAAK,EAAE,GARP;AASAC,UAAAA,MAAM,EAAE;AATR;AADF;AAAA;AAAA;AAAA;AAAA,cADA,eAcA;AAAQ,QAAA,GAAG,EAAEd,SAAb;AACA,QAAA,KAAK,EACH;AACEqC,UAAAA,QAAQ,EAAC,UADX;AAEEC,UAAAA,UAAU,EAAE,MAFd;AAGEC,UAAAA,WAAW,EAAE,MAHf;AAIEC,UAAAA,IAAI,EAAE,CAJR;AAKEC,UAAAA,KAAK,EAAE,CALT;AAMEC,UAAAA,SAAS,EAAE,QANb;AAOEC,UAAAA,MAAM,EAAE,CAPV;AAQE9B,UAAAA,KAAK,EAAE,GART;AASEC,UAAAA,MAAM,EAAE;AATV;AAFF;AAAA;AAAA;AAAA;AAAA,cAdA;AAAA;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,UADF;AAiCD;;GA5JQhB,G;;KAAAA,G;AA8JT,eAAeA,GAAf","sourcesContent":["//install dependencies DONE\n//import dependiencies DONE\n//setup webcam and canvas DONE\n//define references to those DONE\n//load facemesh DONE\n//detect function DONE\n//drawing utilities\n//load triangulation\n//setup triangle path\n//setup point drawing\n//add drawmesh to detect function\n\nimport {useRef} from 'react';\n//import logo from './logo.svg';\nimport './App.css';\nimport * as tf from \"@tensorflow/tfjs\";\nimport * as faceLandmarksDetection from '@tensorflow-models/face-landmarks-detection';\n//at one point i was suppose to use facemesh but this no longer is supported\n//import * as facemesh from \"@tensorflow-models/facemesh\";\nimport Webcam from \"react-webcam\";\nimport {drawMesh} from \"./utilities\"\n\n\nfunction App() {\n  //set up references\n  const webcamRef = useRef(null);\n  const canvasRef = useRef (null);\n\n\n  // load facemesh\n  const runFacemesh = async () =>{\n    if(webcamRef !== null){\n    const net = await faceLandmarksDetection.load(faceLandmarksDetection.SupportedPackages.mediapipeFacemesh);\n\n    setInterval(async ()=>{\n      if(webcamRef){\n      await main(net);\n      }\n    },150)\n  }\n  };\n/*\n  // detect function\n  const detect = async(net) =>{\n    //check to make sure webcame is ready for detection\n    if(\n      typeof webcamRef.currrent !== \"undefined\" && webcamRef.current !== null && webcamRef.current.video.readyState===4){\n        //get video properties\n        const video = webcamRef.current.video;\n        const videoWidth = webcamRef.current.video.videoWidth;\n        const videoHeight = webcamRef.current.video.videoHeight;\n        //set video width\n        webcamRef.current.video.width = videoWidth;\n        webcamRef.current.video.height = videoHeight\n        //set canvas width\n        canvasRef.current.width = videoWidth;\n        canvasRef.current.height = videoHeight;\n        //make detections\n        const face = await net.estimateFaces(video);\n        if (face){\n          console.log(face)\n        }\n        // get canvas context for drawing\n      }\n  }\n\n  runFacemesh();\n  */\n  async function main(model) {\n      const video = webcamRef.current.video;\n        const videoWidth = webcamRef.current.video.videoWidth;\n        const videoHeight = webcamRef.current.video.videoHeight;\n        //set video width\n        webcamRef.current.video.width = videoWidth;\n        webcamRef.current.video.height = videoHeight\n        //set canvas width\n        canvasRef.current.width = videoWidth;\n        canvasRef.current.height = videoHeight;\n    // Load the MediaPipe Facemesh package.\n    if (webcamRef!==null){\n\n    // Pass in a video stream (or an image, canvas, or 3D tensor) to obtain an\n    // array of detected faces from the MediaPipe graph. If passing in a video\n    // stream, a single prediction per frame will be returned.\n    const predictions = await model.estimateFaces({\n      input: video, predictIrises:true\n    });\n    predictions.forEach(prediction=>{\n      //console.log(prediction.scaledMesh)\n    }\n    )\n    if (predictions.length > 0) {\n      const ctx = canvasRef.current.getContext(\"2d\")\n    drawMesh(predictions, ctx)\n      /*\n      `predictions` is an array of objects describing each detected face, for example:\n\n      [\n        {\n          faceInViewConfidence: 1, // The probability of a face being present.\n          boundingBox: { // The bounding box surrounding the face.\n            topLeft: [232.28, 145.26],\n            bottomRight: [449.75, 308.36],\n          },\n          mesh: [ // The 3D coordinates of each facial landmark.\n            [92.07, 119.49, -17.54],\n            [91.97, 102.52, -30.54],\n            ...\n          ],\n          scaledMesh: [ // The 3D coordinates of each facial landmark, normalized.\n            [322.32, 297.58, -17.54],\n            [322.18, 263.95, -30.54]\n          ],\n          annotations: { // Semantic groupings of the `scaledMesh` coordinates.\n            silhouette: [\n              [326.19, 124.72, -3.82],\n              [351.06, 126.30, -3.00],\n              ...\n            ],\n            ...\n          }\n        }\n      ]\n      */\n\n      for (let i = 0; i < predictions.length; i++) {\n        const keypoints = predictions[i].scaledMesh;\n        const header = document.getElementById('webcam');\n        const elementPosition = header.getBoundingClientRect()\n        console.log(elementPosition)\n\n        // Log facial keypoints.\n        for (let i = 0; i < keypoints.length; i++) {\n          const [x, y, z] = keypoints[i];\n\n        //  console.log(`Keypoint ${i}: [${x}, ${y}, ${z}]`);\n        }\n      }\n    }\n  }\n  }\n  if (webcamRef){\n  runFacemesh();\n  }\n\n\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\n      <Webcam id=\"webcam\" ref={webcamRef} style={\n        {\n        position:\"absolute\",\n        marginLeft: \"auto\",\n        marginRight: \"auto\",\n        left: 0,\n        right: 0,\n        textAlign: \"center\",\n        zIndex: 9,\n        width: 640,\n        height: 480\n        }\n      }/>\n      <canvas ref={canvasRef}\n      style={\n        {\n          position:\"absolute\",\n          marginLeft: \"auto\",\n          marginRight: \"auto\",\n          left: 0,\n          right: 0,\n          textAlign: \"center\",\n          zIndex: 9,\n          width: 640,\n          height: 480\n          }\n      } />\n      </header>\n    </div>\n  );\n}\n\nexport default App;\n"]},"metadata":{},"sourceType":"module"}